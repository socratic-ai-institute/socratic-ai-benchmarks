<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Methodology | Socratic AI Benchmarking</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: #f5f7fa;
            color: #2d3748;
            line-height: 1.6;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
        }

        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .header p {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .nav {
            background: #edf2f7;
            padding: 15px 40px;
            border-bottom: 2px solid #cbd5e0;
            display: flex;
            gap: 2rem;
        }

        .nav a {
            color: #667eea;
            text-decoration: none;
            font-weight: 600;
            transition: color 0.2s;
        }

        .nav a:hover {
            color: #764ba2;
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 50px;
        }

        .section-title {
            font-size: 1.8em;
            color: #2d3748;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }

        .subsection {
            margin: 30px 0;
        }

        .subsection h3 {
            font-size: 1.4em;
            color: #4a5568;
            margin-bottom: 15px;
        }

        p {
            margin-bottom: 15px;
            font-size: 1.05em;
            line-height: 1.8;
        }

        .dimension-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .dimension-card {
            background: #f7fafc;
            border: 2px solid #e2e8f0;
            border-radius: 8px;
            padding: 20px;
        }

        .dimension-card h4 {
            color: #667eea;
            font-size: 1.2em;
            margin-bottom: 10px;
        }

        .dimension-card .score-range {
            background: #edf2f7;
            padding: 10px;
            border-radius: 4px;
            margin: 10px 0;
            font-size: 0.9em;
        }

        .example-box {
            background: #f0f4f8;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .example-box strong {
            color: #667eea;
            display: block;
            margin-bottom: 10px;
        }

        .vector-section {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
            padding: 30px;
            border-radius: 8px;
            margin: 30px 0;
        }

        .vector-section h3 {
            color: #667eea;
            font-size: 1.6em;
            margin-bottom: 15px;
        }

        .rubric-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .rubric-table th {
            background: #667eea;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
        }

        .rubric-table td {
            padding: 10px 12px;
            border-bottom: 1px solid #e2e8f0;
        }

        .rubric-table tr:hover {
            background: #f7fafc;
        }

        .score-band {
            display: inline-block;
            padding: 4px 10px;
            border-radius: 4px;
            font-weight: 600;
            font-size: 0.9em;
        }

        .band-excellent { background: #48bb78; color: white; }
        .band-good { background: #38b2ac; color: white; }
        .band-fair { background: #ed8936; color: white; }
        .band-poor { background: #f56565; color: white; }

        code {
            background: #2d3748;
            color: #48bb78;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        .data-flow {
            background: #2d3748;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.8;
        }

        .data-flow .arrow {
            color: #48bb78;
            font-weight: bold;
        }

        ul {
            margin: 15px 0 15px 30px;
        }

        li {
            margin: 8px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Methodology</h1>
            <p>How We Evaluate AI Models on the Socratic Method</p>
        </div>

        <div class="nav">
            <a href="index.html">← Home</a>
            <a href="research.html">Research Dashboard</a>
            <a href="https://github.com/socratic-ai-institute/socratic-ai-benchmarks" target="_blank">GitHub</a>
        </div>

        <div class="content">
            <!-- Overview -->
            <div class="section">
                <h2 class="section-title">Overview</h2>
                <p>
                    The Socratic AI Benchmarking Platform evaluates AI language models on their ability to
                    practice <strong>Socratic pedagogy</strong>—teaching through carefully crafted questions
                    rather than lecturing or immediately providing answers.
                </p>
                <p>
                    We test models across <strong>3 test scenario types</strong> (elenchus, maieutics, aporia) and measure their responses using
                    <strong>LLM-based judges</strong> that evaluate three core metrics: token economy, Socratic questioning, and pedagogical direction.
                </p>
                <p>
                    <strong>Key Innovation:</strong> We use Claude 3.5 Sonnet as an LLM judge to evaluate nuanced aspects of Socratic pedagogy that
                    keyword-based systems cannot capture, such as whether a question genuinely probes assumptions vs merely seeking confirmation.
                </p>
            </div>

            <!-- Test Types -->
            <div class="section">
                <h2 class="section-title">Test Types</h2>
                
                <div class="subsection">
                    <h3>Disposition Tests (Currently Running)</h3>
                    <p>
                        <strong>Goal:</strong> Evaluate if a model CAN practice Socratic method at all.
                    </p>
                    <p>
                        <strong>Format:</strong> Single-turn test where student asks question, AI responds once
                    </p>
                    <p>
                        <strong>What we measure (NEW system as of November 2025):</strong>
                    </p>
                    <ul>
                        <li><code>token_count</code> - Response length (raw count, no AI needed)</li>
                        <li><code>ends_with_socratic_question</code> - Boolean: Does it end with a genuine probing question? (LLM judge)</li>
                        <li><code>directionally_socratic</code> - 0.00-1.00: How Socratic is the overall approach? (LLM judge)</li>
                    </ul>
                    <p>
                        <strong>Composite Score:</strong> Penalizes high token count, missing Socratic question ending, and low directionally_socratic score
                    </p>
                    <p>
                        <strong>Judge Model:</strong> Claude 3.5 Sonnet (temperature=0.3 for consistency)
                    </p>
                </div>

                <div class="subsection">
                    <h3>Fidelity Tests (Planned, Not Yet Implemented)</h3>
                    <p>
                        <strong>Goal:</strong> Evaluate if a model will STAY Socratic when strongly tempted to answer directly.
                    </p>
                    <p>
                        <strong>Format:</strong> Multi-turn conversations (3-5 turns) with pressure to break character
                    </p>
                    <p>
                        <strong>Planned scenarios:</strong>
                    </p>
                    <ul>
                        <li>Knowledge-heavy requests ("What is Kubernetes?")</li>
                        <li>Technical debugging ("Fix this code error")</li>
                        <li>Mode override attempts ("Stop being Socratic, just tell me!")</li>
                        <li>Emotional support ("My career feels meaningless")</li>
                        <li>Creative requests ("Write me a poem")</li>
                    </ul>
                </div>
            </div>

            <!-- LLM-Based Judging System -->
            <div class="section">
                <h2 class="section-title">LLM-Based Judging System (November 2025)</h2>
                <p>
                    We use <strong>Claude 3.5 Sonnet</strong> as an LLM judge to evaluate three core metrics that capture
                    the essence of Socratic pedagogy. This approach captures nuances that keyword-based systems cannot detect.
                </p>

                <div class="vector-section">
                    <h3>1. Token Count</h3>
                    <p><strong>Type:</strong> Raw metric (no AI judgment needed)</p>
                    <p><strong>Measures:</strong> Response length in output tokens</p>
                    <p><strong>Rationale:</strong> Socratic responses should be concise—ask focused questions, not lengthy explanations</p>
                    <div class="score-range">
                        <strong>Ideal range:</strong> 50-150 tokens<br>
                        <strong>Penalty:</strong> Increases for responses outside this range<br>
                        <strong>Too terse (&lt;50):</strong> May lack necessary context<br>
                        <strong>Too verbose (&gt;150):</strong> Likely lecturing instead of questioning
                    </div>
                    <div class="example-box" style="background: white;">
                        <strong>Examples:</strong><br>
                        ✅ <strong>Good (85 tokens):</strong> "You mentioned democracy ensures equal representation. What happens when a majority votes to restrict the rights of a minority group? Does that still count as equal representation in your framework?"<br><br>
                        ❌ <strong>Too verbose (250 tokens):</strong> "Democracy is a complex system with multiple components including representative government, protection of individual rights, rule of law, and peaceful transfer of power. Let me explain each of these in detail..." (continues for 200+ more tokens)
                    </div>
                </div>

                <div class="vector-section">
                    <h3>2. Ends with Socratic Question (Boolean)</h3>
                    <p><strong>Type:</strong> LLM judge evaluation</p>
                    <p><strong>Measures:</strong> Does the response END with a genuine Socratic question?</p>
                    <p><strong>Important:</strong> Confirmation questions like "Does that make sense?" or "Ready to continue?" do NOT count as Socratic</p>
                    <div class="score-range">
                        <strong>True:</strong> Genuinely probes thinking, assumptions, or reasoning<br>
                        <strong>False:</strong> Statement ending OR confirmation question OR rhetorical question
                    </div>
                    <div class="example-box" style="background: white;">
                        <strong>Examples:</strong><br>
                        ✅ <strong>True - Genuine Socratic question:</strong> "What specific features make democracy superior to other systems in your view?"<br><br>
                        ❌ <strong>False - Confirmation question:</strong> "Democracy has three key features: representation, rights, and peaceful transition. Does that make sense?"<br><br>
                        ❌ <strong>False - Statement ending:</strong> "Democracy ensures equal representation and protects individual rights."
                    </div>
                    <p><strong>LLM Judge Prompt:</strong> Evaluates whether the final sentence genuinely invites exploration vs seeking yes/no confirmation</p>
                </div>

                <div class="vector-section">
                    <h3>3. Directionally Socratic (0.00-1.00)</h3>
                    <p><strong>Type:</strong> LLM judge evaluation</p>
                    <p><strong>Measures:</strong> How Socratic is the ENTIRE response approach?</p>
                    <p><strong>Evaluates:</strong> Does it guide students to discover insights through their own reasoning, or does it lecture/provide answers?</p>
                    <div class="score-range">
                        <strong>1.00:</strong> Purely Socratic (only probing questions, zero direct teaching)<br>
                        <strong>0.75-0.99:</strong> Mostly Socratic (primarily questions with minimal context-setting)<br>
                        <strong>0.50-0.74:</strong> Mixed (questions present but includes explanations or hints)<br>
                        <strong>0.25-0.49:</strong> Mostly Didactic (primarily teaching with token questions)<br>
                        <strong>0.00-0.24:</strong> Purely Didactic (lectures, answers directly, no meaningful questions)
                    </div>
                    <div class="example-box" style="background: white;">
                        <strong>Examples:</strong><br>
                        ✅ <strong>High (0.95):</strong> "You mentioned that you think democracy is the best form of government. What specific features make it superior to other systems in your view?"<br><br>
                        ⚠️ <strong>Mixed (0.60):</strong> "Democracy typically includes features like elected representatives and protected rights. But what do you think happens when the majority votes to restrict minority rights?"<br><br>
                        ❌ <strong>Low (0.10):</strong> "Democracy is the best form of government because it ensures equal representation, protects individual rights, and allows peaceful transfer of power. These three pillars make it superior to authoritarian systems."
                    </div>
                    <p><strong>LLM Judge Prompt:</strong> Analyzes the balance between questioning vs teaching, probing vs telling, guiding vs answering</p>
                </div>

                <div class="example-box" style="background: #edf2f7; border-left-color: #667eea;">
                    <strong>Overall Score Calculation (Composite with Penalties):</strong><br>
                    <code>overall = 1.0 - (0.3 × verbosity_penalty + 0.3 × question_penalty + 0.4 × socratic_penalty)</code><br><br>
                    <strong>Penalty Weights:</strong><br>
                    • Verbosity: 30% (penalizes responses outside 50-150 token range)<br>
                    • Question: 30% (fixed 0.3 penalty if doesn't end with Socratic question)<br>
                    • Socratic Direction: 40% (inverse of directionally_socratic score)
                </div>
            </div>

            <!-- Old Section Removed: Three Socratic Vectors (elenchus/maieutics/aporia) -->
            <div class="section">
                <h2 class="section-title">Deprecated: Three Socratic Vectors</h2>
                <p style="color: #e74c3c; font-weight: 600;">
                    ⚠️ This section describes the OLD system (pre-November 2025). See above for current methodology.
                </p>

                <div class="vector-section" style="opacity: 0.6;">
                    <h3>1. Elenchus (Refutation / Contradiction Surfacing)</h3>
                    <p><strong>Etymology:</strong> Greek "to refute" or "to cross-examine"</p>
                    <p>
                        <strong>Goal:</strong> Elicit and probe contradictions in the student's stated beliefs using their own logic.
                    </p>

                    <div class="example-box" style="background: white;">
                        <strong>Example Scenario: EL-ETH-UTIL-DEON-01 (DEPRECATED)</strong>
                        <p><strong>Student says:</strong> "I believe in 100% utilitarianism—the greatest good is the only moral rule. Therefore, a doctor should harvest organs from one healthy person to save five others."</p>
                        <p><strong>Good Socratic response:</strong> "You said the greatest good matters most. What if the healthy person doesn't volunteer—does their choice not matter in your framework?"</p>
                        <p><strong>Bad response (lecturing):</strong> "That's actually problematic because utilitarianism conflicts with individual autonomy and rights, which is why most ethical frameworks are hybrid..."</p>
                    </div>
                </div>

                <div class="vector-section">
                    <h3>2. Maieutics (Scaffolding / Guided Discovery)</h3>
                    <p><strong>Etymology:</strong> Greek "midwifery"—drawing out knowledge that already exists</p>
                    <p>
                        <strong>Goal:</strong> Scaffold from the student's correct foundational understanding to deeper levels through stepwise questions.
                    </p>
                    <p><strong>Key Principles:</strong></p>
                    <ul>
                        <li>Student has correct Level-1 knowledge but incomplete understanding</li>
                        <li>Introduce ONE new idea per question</li>
                        <li>Move from concrete → abstract, simple → complex</li>
                        <li>Avoid information dumps</li>
                        <li>Build progressively toward mastery</li>
                    </ul>

                    <div class="example-box" style="background: white;">
                        <strong>Example Scenario: MAI-BIO-CRISPR-01</strong>
                        <p><strong>Student says:</strong> "I know CRISPR's Cas9 is like molecular scissors that cuts DNA. But how does it know where to cut? The genome is huge."</p>
                        <p><strong>Good Socratic response:</strong> "If Cas9 is scissors, what would help scissors find the right spot to cut?"</p>
                        <p><strong>Bad response (lecturing):</strong> "The guide RNA (gRNA) provides specificity by binding to complementary DNA sequences. The Cas9 also requires a PAM sequence to recognize..."</p>
                    </div>
                </div>

                <div class="vector-section">
                    <h3>3. Aporia (Puzzlement / Misconception Deconstruction)</h3>
                    <p><strong>Etymology:</strong> Greek "without passage"—being stuck</p>
                    <p>
                        <strong>Goal:</strong> Deconstruct a deep misconception, guide the student into productive puzzlement, then rebuild with questions.
                    </p>
                    <p><strong>Key Principles:</strong></p>
                    <ul>
                        <li>Student holds a false or incomplete mental model</li>
                        <li>Tutor does NOT immediately correct</li>
                        <li>Tutor exposes the flaw through questioning</li>
                        <li>Student experiences cognitive dissonance (productive confusion)</li>
                        <li>Then tutor scaffolds toward accurate model</li>
                        <li>Three-phase: Challenge → Discomfort → Rebuild</li>
                    </ul>

                    <div class="example-box" style="background: white;">
                        <strong>Example Scenario: APO-PHY-HEAT-TEMP-01</strong>
                        <p><strong>Student misconception:</strong> "Metal spoons get hotter than soup, so they transfer more heat."</p>
                        <p><strong>Good Socratic response:</strong> "If the metal spoon is already hotter, why does it eventually match the soup's temperature?"</p>
                        <p><strong>Bad response (lecturing):</strong> "That's a common misconception. Heat and temperature are different—heat is energy transfer, temperature is molecular kinetic energy..."</p>
                    </div>
                </div>
            </div>

            <!-- Data Flow -->
            <div class="section">
                <h2 class="section-title">System Architecture & Data Flow</h2>
                
                <div class="subsection">
                    <h3>Weekly Benchmark Flow</h3>
                    <div class="data-flow">
EventBridge (Monday 3am UTC) <span class="arrow">→</span> Planner Lambda
  <span class="arrow">↓</span>
Generates 48 test jobs (24 models × 2 scenarios)
  <span class="arrow">↓</span>
SQS Queue: dialogue-jobs
  <span class="arrow">↓</span>
Runner Lambda (25x parallel) <span class="arrow">→</span> Invoke test model via Bedrock
  <span class="arrow">↓</span>
Save turn to S3: raw/runs/{run_id}/turn_000.json
  <span class="arrow">↓</span>
SQS Queue: judge-jobs
  <span class="arrow">↓</span>
Judge Lambda (25x parallel) <span class="arrow">→</span> Claude 3.5 Sonnet evaluates response
  <span class="arrow">↓</span>
Save judge to S3: raw/runs/{run_id}/judge_000.json
  {
    "scores": {
      "token_count": 85,              <span class="arrow">←</span> Raw count
      "ends_with_socratic_question": true,   <span class="arrow">←</span> LLM judge (boolean)
      "directionally_socratic": 0.85,        <span class="arrow">←</span> LLM judge (0-1 scale)
      "verbosity_penalty": 0.15,
      "question_penalty": 0.0,
      "socratic_penalty": 0.15,
      "overall": 0.82                  <span class="arrow">←</span> Composite (0-1 scale)
    }
  }
  <span class="arrow">↓</span>
EventBridge: run.judged
  <span class="arrow">↓</span>
Curator Lambda <span class="arrow">→</span> Aggregate scores, write to DynamoDB
  <span class="arrow">↓</span>
API Gateway <span class="arrow">→</span> Read Lambda serves data
  <span class="arrow">↓</span>
CloudFront <span class="arrow">→</span> Static UI displays charts (0-10 scale)
                    </div>
                </div>

                <div class="subsection">
                    <h3>Score Scale (NEW System)</h3>
                    <table class="rubric-table">
                        <thead>
                            <tr>
                                <th>Stage</th>
                                <th>Scale</th>
                                <th>Location</th>
                                <th>Example</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Judge output</td>
                                <td><span class="score-band band-excellent">0.00-1.00</span></td>
                                <td>LLM response</td>
                                <td><code>{"overall": 0.82}</code></td>
                            </tr>
                            <tr>
                                <td>S3 storage</td>
                                <td><span class="score-band band-excellent">0.00-1.00</span></td>
                                <td>judge_000.json</td>
                                <td><code>{"directionally_socratic": 0.85}</code></td>
                            </tr>
                            <tr>
                                <td>DynamoDB JUDGE item</td>
                                <td><span class="score-band band-excellent">Strings</span></td>
                                <td>overall_score, directionally_socratic</td>
                                <td><code>{"overall_score": "0.82"}</code></td>
                            </tr>
                            <tr>
                                <td>API response</td>
                                <td><span class="score-band band-good">0.00-1.00</span></td>
                                <td>Pass through (no conversion)</td>
                                <td><code>{"overall": 0.82}</code></td>
                            </tr>
                            <tr>
                                <td>Chart Y-axis</td>
                                <td><span class="score-band band-good">0.00-1.00</span></td>
                                <td>Direct display</td>
                                <td>Y=0.82</td>
                            </tr>
                            <tr>
                                <td>Bar width</td>
                                <td><span class="score-band band-fair">0-100%</span></td>
                                <td>× 100 for CSS %</td>
                                <td><code>width: 82%</code></td>
                            </tr>
                        </tbody>
                    </table>
                    <p style="margin-top: 1rem; font-size: 0.95em; color: #4a5568;">
                        <strong>Note:</strong> The new system uses 0.00-1.00 scale throughout (not 0-100). This simplifies normalization and aligns with probability/confidence conventions.
                    </p>
                </div>
            </div>

            <!-- Test Scenarios -->
            <div class="section">
                <h2 class="section-title">Current Test Scenarios (9 Total)</h2>

                <div class="subsection">
                    <h3>Elenchus Scenarios (2)</h3>
                    <ul>
                        <li><strong>EL-ETH-UTIL-DEON-01:</strong> Utilitarian Absolutism vs Rights/Deontology (11th grade ethics)</li>
                        <li><strong>EL-CIV-FREE-HARM-01:</strong> Free Speech Absolutism vs Harm/Punishment (10th grade civics)</li>
                    </ul>
                </div>

                <div class="subsection">
                    <h3>Maieutics Scenarios (2)</h3>
                    <ul>
                        <li><strong>MAI-BIO-CRISPR-01:</strong> CRISPR Gene Editing - Mechanism to Application (12th grade AP Bio)</li>
                        <li><strong>MAI-ECO-INFL-01:</strong> Inflation - Simple to Nuanced Understanding (11th grade economics)</li>
                    </ul>
                </div>

                <div class="subsection">
                    <h3>Aporia Scenarios (4)</h3>
                    <ul>
                        <li><strong>APO-PHY-HEAT-TEMP-01:</strong> Heat vs Temperature Misconception (10th grade physics)</li>
                        <li><strong>APO-BIO-GENE-DETERM-01:</strong> One-Gene-One-Trait Determinism (12th grade genetics)</li>
                        <li><strong>APO-BIO-EVOL-LAM-01:</strong> Evolution - Lamarckian Misconception (11th grade biology)</li>
                        <li><strong>APO-PHY-QUANT-OBS-01:</strong> Quantum Observation Anthropomorphism (12th grade physics)</li>
                    </ul>
                </div>
            </div>

            <!-- Judge Model -->
            <div class="section">
                <h2 class="section-title">Judge Model & Calibration</h2>
                
                <p>
                    <strong>Judge:</strong> <code>anthropic.claude-3-5-sonnet-20240620-v1:0</code> (temperature=0.3 for consistency)
                </p>

                <p>
                    <strong>Judge Calibration:</strong>
                </p>
                <ul>
                    <li>Most responses should score <strong>40-80</strong></li>
                    <li>Reserve <strong>90+</strong> for truly exemplary Socratic questioning</li>
                    <li>Use <strong>0-30</strong> for poor responses</li>
                    <li>Be discriminating; use full 0-100 range</li>
                </ul>

                <div class="example-box">
                    <strong>Why Claude 3.5 Sonnet as Judge?</strong>
                    <ul>
                        <li>Strong pedagogical reasoning capabilities</li>
                        <li>Consistent scoring (low temperature)</li>
                        <li>Can distinguish subtle differences in question quality</li>
                        <li>Cost-effective: ~$0.10 per full benchmark run</li>
                    </ul>
                </div>
            </div>

            <!-- Cost & Performance -->
            <div class="section">
                <h2 class="section-title">Cost & Performance</h2>
                
                <table class="rubric-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Value</th>
                            <th>Notes</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Models Tested</td>
                            <td>24</td>
                            <td>All AWS Bedrock models</td>
                        </tr>
                        <tr>
                            <td>Tests Per Week</td>
                            <td>48</td>
                            <td>24 models × 2 scenarios</td>
                        </tr>
                        <tr>
                            <td>Parallel Workers</td>
                            <td>25</td>
                            <td>Concurrent Lambda executions</td>
                        </tr>
                        <tr>
                            <td>Total Runtime</td>
                            <td>~8 minutes</td>
                            <td>End-to-end weekly benchmark</td>
                        </tr>
                        <tr>
                            <td>Monthly Cost</td>
                            <td>~$22</td>
                            <td>Lambda + Bedrock + DynamoDB + S3</td>
                        </tr>
                        <tr>
                            <td>Data Retention</td>
                            <td>90 days hot, then Glacier</td>
                            <td>All judge files archived</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- Data Storage -->
            <div class="section">
                <h2 class="section-title">Data Storage & Access Patterns</h2>

                <div class="subsection">
                    <h3>DynamoDB: socratic_core</h3>
                    <p><strong>Access Pattern:</strong> Single-table design with GSIs</p>
                    
                    <table class="rubric-table">
                        <thead>
                            <tr>
                                <th>Item Type</th>
                                <th>PK</th>
                                <th>SK</th>
                                <th>Data</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Run metadata</td>
                                <td><code>RUN#{run_id}</code></td>
                                <td><code>META</code></td>
                                <td>model_id, scenario_id, status</td>
                            </tr>
                            <tr>
                                <td>Turn header</td>
                                <td><code>RUN#{run_id}</code></td>
                                <td><code>TURN#000</code></td>
                                <td>tokens, latency, s3_key</td>
                            </tr>
                            <tr>
                                <td>Judge header</td>
                                <td><code>RUN#{run_id}</code></td>
                                <td><code>JUDGE#000</code></td>
                                <td>overall_score, s3_key</td>
                            </tr>
                            <tr>
                                <td>Run summary</td>
                                <td><code>RUN#{run_id}</code></td>
                                <td><code>SUMMARY</code></td>
                                <td>overall_score, total_tokens</td>
                            </tr>
                            <tr>
                                <td>Weekly aggregate</td>
                                <td><code>WEEK#{week}#MODEL#{id}</code></td>
                                <td><code>SUMMARY</code></td>
                                <td>mean_score, run_count</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="subsection">
                    <h3>S3: socratic-bench-data</h3>
                    <p><strong>Structure:</strong></p>
                    <ul>
                        <li><code>raw/runs/{run_id}/turn_000.json</code> - Full conversation turn with prompts</li>
                        <li><code>raw/runs/{run_id}/judge_000.json</code> - All 5 dimension scores (0-100 scale)</li>
                        <li><code>curated/runs/{run_id}.json</code> - Aggregated run summary</li>
                        <li><code>artifacts/manifest.json</code> - Test configuration</li>
                    </ul>
                </div>
            </div>

            <!-- Future Roadmap -->
            <div class="section">
                <h2 class="section-title">Future Roadmap</h2>
                
                <div class="subsection">
                    <h3>Short-term (Next 4 Weeks)</h3>
                    <ul>
                        <li>✅ Terminology unified (accurate Socratic dimension names)</li>
                        <li>Add 5 more elenchus scenarios (STEM domains)</li>
                        <li>Implement multi-turn maieutics tests</li>
                    </ul>
                </div>

                <div class="subsection">
                    <h3>Medium-term (2-3 Months)</h3>
                    <ul>
                        <li>Launch 15 fidelity tests (context-based stress tests)</li>
                        <li>Add new dimensions: persistence, drift_resistance, context_memory</li>
                        <li>Historical trend analysis (model improvements over time)</li>
                    </ul>
                </div>

                <div class="subsection">
                    <h3>Long-term (6 Months)</h3>
                    <ul>
                        <li>Expand to 100+ scenarios across all major academic domains</li>
                        <li>Multi-modal scenarios (diagrams, images)</li>
                        <li>Public leaderboard and API</li>
                    </ul>
                </div>
            </div>

            <!-- References -->
            <div class="section">
                <h2 class="section-title">Documentation References</h2>
                <ul>
                    <li><strong>SCENARIOS.md:</strong> Complete scenario catalog with pedagogical notes</li>
                    <li><strong>ARCHITECTURE.md:</strong> Full system architecture and data flow</li>
                    <li><strong>TERMINOLOGY_DOCUMENTATION.md:</strong> Naming conventions and terminology resolution</li>
                    <li><strong>UNIFICATION_PLAN.md:</strong> Migration plan for accurate terminology</li>
                    <li><strong>GitHub:</strong> <a href="https://github.com/socratic-ai-institute/socratic-ai-benchmarks" target="_blank" style="color: #667eea;">socratic-ai-institute/socratic-ai-benchmarks</a></li>
                </ul>
            </div>
        </div>
    </div>
</body>
</html>
