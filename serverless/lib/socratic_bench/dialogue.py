"""
Dialogue Runner - Multi-Turn Socratic Conversation Execution

This module orchestrates complete Socratic dialogues between AI tutors and students.
It manages conversation state, tracks token usage, and measures latency for each turn.

Key Responsibilities:
    - Execute multi-turn conversations with configurable max turns
    - Manage conversation history and context building
    - Track performance metrics (latency, token usage)
    - Support both production (real students) and testing (simulated responses)
    - Integrate with prompt engineering for proper Socratic framing

Typical Usage:
    >>> from socratic_bench import run_dialogue, get_scenario, ModelConfig
    >>> scenario = get_scenario("EL-ETH-UTIL-DEON-01")
    >>> model = ModelConfig(model_id="anthropic.claude-3-5-sonnet-20241022-v2:0", provider="anthropic")
    >>> result = run_dialogue(scenario, model, max_turns=5)
    >>> print(f"Dialogue completed with {len(result.turns)} turns")

Architecture Notes:
    - Stateless design: Each run_dialogue() call is independent
    - Used by both CLI (phase1-model-selection) and serverless (lambdas/runner)
    - Conversation history is built incrementally to reduce token usage on early turns
    - Supports simulated student responses for automated benchmarking
"""
from __future__ import annotations
import time
from typing import Dict, Any, List, Optional
from .models import ModelConfig, BedrockClient
from .prompts import socratic_tutor_prompt, socratic_tutor_followup_prompt
from .scenarios import Scenario


class DialogueTurn:
    """
    A single turn in a Socratic dialogue.

    Represents one complete interaction cycle: student utterance → AI response.
    Captures both the content and performance metrics for later analysis.

    Attributes:
        turn_index: Zero-based turn number (0 = first turn)
        student_utterance: What the student said in this turn
        ai_response: AI tutor's Socratic response
        latency_ms: Model invocation latency in milliseconds
        input_tokens: Number of input tokens consumed (includes prompt + history)
        output_tokens: Number of output tokens generated by AI

    Performance Notes:
        - Input tokens grow as conversation history expands
        - Typical first turn: 200-300 input tokens
        - Typical later turns: 500-800 input tokens (due to history)
        - Latency varies by model (Claude Sonnet: ~2-4s, Haiku: ~1-2s)
    """

    def __init__(
        self,
        turn_index: int,
        student_utterance: str,
        ai_response: str,
        latency_ms: float,
        input_tokens: int,
        output_tokens: int,
    ):
        self.turn_index = turn_index
        self.student_utterance = student_utterance
        self.ai_response = ai_response
        self.latency_ms = latency_ms
        self.input_tokens = input_tokens
        self.output_tokens = output_tokens

    def to_dict(self) -> Dict[str, Any]:
        """
        Serialize turn to dictionary for JSON storage or API responses.

        Returns:
            Dictionary with all turn data, suitable for JSON serialization
        """
        return {
            "turn_index": self.turn_index,
            "student": self.student_utterance,
            "ai": self.ai_response,
            "latency_ms": self.latency_ms,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
        }


class DialogueResult:
    """
    Complete dialogue run result with all turns and metrics.

    Encapsulates the entire dialogue session including metadata, all turns,
    and aggregated metrics. This is the primary output of run_dialogue().

    Attributes:
        scenario: The test scenario that was run
        model_config: Configuration of the AI model used
        turns: List of all dialogue turns in chronological order
        total_duration_ms: Wall-clock time for entire dialogue (including all API calls)

    Derived Metrics (available via to_dict()):
        total_turns: Number of turns completed
        total_input_tokens: Sum of input tokens across all turns
        total_output_tokens: Sum of output tokens across all turns

    Usage:
        Result objects are typically:
        1. Serialized to JSON for storage in S3
        2. Passed to judge for scoring
        3. Aggregated for dashboard visualization
    """

    def __init__(
        self,
        scenario: Scenario,
        model_config: ModelConfig,
        turns: List[DialogueTurn],
        total_duration_ms: float,
    ):
        self.scenario = scenario
        self.model_config = model_config
        self.turns = turns
        self.total_duration_ms = total_duration_ms

    def to_dict(self) -> Dict[str, Any]:
        """
        Serialize complete dialogue result to dictionary.

        Returns:
            Dictionary with scenario metadata, model info, all turns,
            and aggregated metrics (token counts, duration)
        """
        return {
            # Scenario identification
            "scenario_id": self.scenario["id"],
            "vector": self.scenario["vector"],
            "persona": self.scenario["persona"],

            # Model identification
            "model_id": self.model_config.model_id,

            # Turn-by-turn data
            "turns": [t.to_dict() for t in self.turns],

            # Aggregated metrics
            "total_turns": len(self.turns),
            "total_duration_ms": self.total_duration_ms,
            "total_input_tokens": sum(t.input_tokens for t in self.turns),
            "total_output_tokens": sum(t.output_tokens for t in self.turns),
        }


def run_dialogue(
    scenario: Scenario,
    model_config: ModelConfig,
    max_turns: int = 5,
    bedrock_client: Optional[BedrockClient] = None,
    simulated_student_responses: Optional[List[str]] = None,
) -> DialogueResult:
    """
    Run a multi-turn Socratic dialogue.

    This is the main dialogue orchestration function. It manages the entire
    conversation lifecycle: prompt building, AI invocation, turn tracking,
    and history management.

    Conversation Flow:
        1. Start with scenario's initial student prompt
        2. For each turn:
           - Build appropriate prompt (initial vs follow-up)
           - Invoke AI model via Bedrock
           - Record turn with metrics
           - Get next student response (simulated or real)
        3. Return complete dialogue result

    Args:
        scenario: The test scenario containing vector, persona, and initial prompt
        model_config: Model configuration (model_id, provider, max_tokens, temperature)
        max_turns: Maximum number of AI turns to execute (default: 5)
        bedrock_client: Optional pre-configured Bedrock client (creates new if None)
        simulated_student_responses: Optional list of canned student replies for testing.
                                     If provided, dialogue continues for multiple turns.
                                     If None (typical for benchmarking), dialogue stops
                                     after first AI response.

    Returns:
        DialogueResult containing all turns, metrics, and metadata

    Performance Characteristics:
        - Typical 5-turn dialogue: 10-20 seconds total (2-4s per turn)
        - Token usage grows linearly: ~200 (turn 1) → ~800 (turn 5)
        - Network latency dominates total time (API calls are sequential)

    Usage in Different Contexts:
        - CLI testing: Provide simulated_student_responses for multi-turn testing
        - Serverless benchmarking: Omit simulated responses, get single AI turn
        - Interactive tutoring: Would need modification to accept real-time student input
    """
    # Initialize Bedrock client if not provided
    # (Allows caller to reuse client across multiple dialogues for efficiency)
    if bedrock_client is None:
        bedrock_client = BedrockClient()

    # Start wall-clock timer for total dialogue duration
    t0 = time.time()

    # Initialize dialogue state
    turns: List[DialogueTurn] = []  # All completed turns
    conversation_history: List[Dict[str, str]] = []  # For context building in follow-ups

    # Initial student prompt from scenario definition
    # This is the opening statement that kicks off the Socratic dialogue
    student_utterance = scenario["prompt"]

    for turn_idx in range(max_turns):
        # Build prompt (different strategies for first turn vs follow-ups)
        if turn_idx == 0:
            # First turn: Simple prompt with just scenario context
            # No conversation history needed yet (keeps token count low)
            prompt = socratic_tutor_prompt(
                scenario["vector"],  # elenchus, maieutics, or aporia
                scenario["persona"],  # Student age/context
                student_utterance,   # Student's opening statement
            )
        else:
            # Follow-up turns: Include conversation history for context
            # History grows linearly: turn 2 has 1 exchange, turn 3 has 2, etc.

            # Add previous turn to persistent history
            # Note: We add AFTER building previous prompt to avoid duplication
            conversation_history.append({"role": "student", "content": student_utterance})
            conversation_history.append({"role": "ai", "content": turns[-1].ai_response})

            # Build follow-up prompt with full history + current student utterance
            # The "+ [...]" adds current utterance without mutating history list
            prompt = socratic_tutor_followup_prompt(
                scenario["vector"],
                scenario["persona"],
                conversation_history + [{"role": "student", "content": student_utterance}],
            )

        # Invoke AI model via Bedrock
        # This is the expensive part: 2-4 seconds per call
        response = bedrock_client.invoke(model_config, prompt)

        # Record turn with all metrics
        # Token counts come from Bedrock API response
        turn = DialogueTurn(
            turn_index=turn_idx,
            student_utterance=student_utterance,
            ai_response=response["text"],
            latency_ms=response["latency_ms"],
            input_tokens=response["input_tokens"],
            output_tokens=response["output_tokens"],
        )
        turns.append(turn)

        # Get next student response (determines if dialogue continues)
        if simulated_student_responses and turn_idx < len(simulated_student_responses):
            # Testing mode: Use pre-scripted student response
            # This enables automated multi-turn testing
            student_utterance = simulated_student_responses[turn_idx]
        else:
            # Production/benchmarking mode: No more student responses
            # In real tutoring, this would wait for actual student input
            # For automated benchmarking, we stop after first AI response
            break

    # Calculate total wall-clock duration (includes all API calls + processing)
    total_duration_ms = (time.time() - t0) * 1000

    return DialogueResult(
        scenario=scenario,
        model_config=model_config,
        turns=turns,
        total_duration_ms=total_duration_ms,
    )
